{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = tf.constant([[1,2,3],[4,5,6]], tf.int32)\n",
    "# b = tf.constant([2,2], tf.int32)\n",
    "# tf.tile(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "class SelfAttention(keras.Model):\n",
    "    def __init__(self, embed_size, head):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.head = head\n",
    "        self.head_dim = embed_size // head\n",
    "\n",
    "        assert (self.embed_size == self.head * self.head_dim), \"Embedding size needs to be divisible by heads\"\n",
    "\n",
    "        self.values = keras.layers.Dense(units=embed_size)\n",
    "        self.keys = keras.layers.Dense(units=embed_size)\n",
    "        self.query = keras.layers.Dense(units=embed_size)\n",
    "        self.fc = keras.layers.Dense(units=embed_size)\n",
    "\n",
    "    def call(self, values, keys, query, mask):\n",
    "        N = tf.shape(query)[0]\n",
    "        value_len, key_len, query_len = tf.shape(values)[1], tf.shape(keys)[1], tf.shape(query)[1]\n",
    "\n",
    "        # Linear transformations\n",
    "        values = self.values(values)  # shape: (N, value_len, embed_size)\n",
    "        keys = self.keys(keys)        # shape: (N, key_len, embed_size)\n",
    "        queries = self.query(query)   # shape: (N, query_len, embed_size)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        values = tf.reshape(values, (N, value_len, self.head, self.head_dim))\n",
    "        keys = tf.reshape(keys, (N, key_len, self.head, self.head_dim))\n",
    "        queries = tf.reshape(queries, (N, query_len, self.head, self.head_dim))\n",
    "\n",
    "        # Compute attention scores\n",
    "        energy = tf.einsum(\"nqhd,nkhd->nhqk\", queries, keys)\n",
    "\n",
    "        # Apply the mask if present\n",
    "        if mask is not None:\n",
    "            mask = tf.cast(mask, dtype=tf.bool)\n",
    "            energy = tf.where(mask, energy, tf.fill(tf.shape(energy), -1e9))\n",
    "\n",
    "        # Attention weights\n",
    "        attention = tf.nn.softmax(energy / tf.sqrt(tf.cast(self.head_dim, dtype=tf.float32)), axis=-1)\n",
    "\n",
    "        # Weighted sum of values\n",
    "        out = tf.einsum(\"nhqk,nkhd->nqhd\", attention, values)\n",
    "        out = tf.reshape(out, (N, query_len, self.head * self.head_dim))\n",
    "\n",
    "        # Final linear layer\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "class TransformerBlock(keras.Model):\n",
    "    def __init__(self, embed_size, head, dropout, forward_expansion):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.attention_layer = SelfAttention(embed_size, head)\n",
    "        self.norm1 = keras.layers.LayerNormalization(axis=-1)\n",
    "        self.norm2 = keras.layers.LayerNormalization(axis=-1)\n",
    "        self.feed_forward = keras.Sequential([\n",
    "            keras.layers.Dense(units=forward_expansion * embed_size, activation=\"relu\"),\n",
    "            keras.layers.Dense(units=embed_size)\n",
    "        ])\n",
    "        self.dropout = keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, value, key, query, mask):\n",
    "        attention = self.attention_layer(value, key, query, mask)\n",
    "        x = self.dropout(self.norm1(attention + query))\n",
    "\n",
    "        forward = self.feed_forward(x)\n",
    "        out = self.dropout(self.norm2(forward + x))\n",
    "        return out\n",
    "\n",
    "class Encoder(keras.Model):\n",
    "    def __init__(self, src_vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, max_length):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.word_embedding = keras.layers.Embedding(input_dim=src_vocab_size, output_dim=embed_size)\n",
    "        self.position_embedding = keras.layers.Embedding(input_dim=max_length, output_dim=embed_size)\n",
    "        self.encoder_layers = [TransformerBlock(embed_size, heads, dropout, forward_expansion) for _ in range(num_layers)]\n",
    "        self.dropout = keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        N = tf.shape(x)[0]\n",
    "        seq_length = tf.shape(x)[1]\n",
    "\n",
    "        # Generate positions\n",
    "        positions = tf.range(start=0, limit=seq_length, delta=1)\n",
    "        positions = tf.expand_dims(positions, axis=0)\n",
    "        positions = tf.tile(positions, [N, 1])\n",
    "\n",
    "        # Apply embeddings\n",
    "        out = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "\n",
    "        # Pass through transformer layers\n",
    "        for layer in self.encoder_layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "\n",
    "        return out\n",
    "\n",
    "class DecoderBlock(keras.Model):\n",
    "    def __init__(self, embed_size, heads, forward_expansion, dropout):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.norm1 = keras.layers.LayerNormalization(axis=-1)\n",
    "        self.attention_layer = SelfAttention(embed_size, heads)\n",
    "        self.transformer_block = TransformerBlock(embed_size, heads, dropout, forward_expansion)\n",
    "        self.dropout = keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, value, key, src_mask, trg_mask):\n",
    "        attention = self.attention_layer(x, x, x, trg_mask)\n",
    "        query = self.dropout(self.norm1(attention + x))\n",
    "        out = self.transformer_block(value, key, query, src_mask)\n",
    "        return out\n",
    "\n",
    "class Decoder(keras.Model):\n",
    "    def __init__(self, trg_vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, max_length):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.word_embedding = keras.layers.Embedding(input_dim=trg_vocab_size, output_dim=embed_size)\n",
    "        self.position_embedding = keras.layers.Embedding(input_dim=max_length, output_dim=embed_size)\n",
    "        self.decoder_layers = [DecoderBlock(embed_size, heads, forward_expansion, dropout) for _ in range(num_layers)]\n",
    "        self.fc_out = keras.layers.Dense(trg_vocab_size)\n",
    "        self.dropout = keras.layers.Dropout(dropout)\n",
    "\n",
    "    def call(self, x, enc_out, src_mask, trg_mask):\n",
    "        N = tf.shape(x)[0]\n",
    "        seq_length = tf.shape(x)[1]\n",
    "\n",
    "        # Generate positions\n",
    "        positions = tf.range(start=0, limit=seq_length)\n",
    "        positions = tf.expand_dims(positions, axis=0)\n",
    "        positions = tf.tile(positions, [N, 1])\n",
    "\n",
    "        x = self.dropout(self.word_embedding(x) + self.position_embedding(positions))\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "        \n",
    "        out = self.fc_out(x)\n",
    "        return out\n",
    "\n",
    "class Transformer(keras.Model):\n",
    "    def __init__(self, src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, embed_size=128,\n",
    "                 num_layers=6, forward_expansion=4, heads=8, dropout=0, max_length=500):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(src_vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, max_length)\n",
    "        self.decoder = Decoder(trg_vocab_size, embed_size, num_layers, heads, forward_expansion, dropout, max_length)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "\n",
    "    def make_src_mask(self, source):\n",
    "        src_mask = tf.cast(tf.math.not_equal(source, self.src_pad_idx), tf.float32)\n",
    "        src_mask = tf.expand_dims(tf.expand_dims(src_mask, axis=1), axis=2) # (batch_size, 1, 1, src_len)\n",
    "        return src_mask\n",
    "    \n",
    "    def make_trg_mask(self, target):\n",
    "        N = tf.shape(target)[0]\n",
    "        target_len = tf.shape(target)[1]\n",
    "\n",
    "        mask = tf.linalg.band_part(input = tf.ones((target_len, target_len)), num_lower = -1, num_upper = 0)    ### Lower triangular matrix\n",
    "        trg_mask = tf.tile(input= tf.expand_dims(input= mask, axis= 0), multiples= [N, 1, 1])       ### (batch_size, trg_len, trg_len)\n",
    "        return trg_mask\n",
    "\n",
    "    def call(self, src, trg):\n",
    "        src_mask = self.make_src_mask(src)\n",
    "        trg_mask = self.make_trg_mask(trg)\n",
    "\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
    "        return out\n",
    "\n",
    "    def train_step(self, data):\n",
    "        # Unpack the data\n",
    "        (src, trg_input), trg_real = data\n",
    "        \n",
    "        # Forward pass\n",
    "        with tf.GradientTape() as tape:\n",
    "            # predictions = self(src, trg_input)  # Model call with src and trg_input\n",
    "            predictions = self.call(src, trg_input)  # Model call with src and trg_input\n",
    "            loss = self.compiled_loss(trg_real, predictions)  # Compute loss\n",
    "        \n",
    "        # Backward pass and apply gradients\n",
    "        trainable_vars = self.trainable_variables\n",
    "        gradients = tape.gradient(loss, trainable_vars)\n",
    "        self.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "\n",
    "        # Return metrics\n",
    "        return {\"loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "src_pad_idx = 0\n",
    "trg_pad_idx = 0\n",
    "src_vocab_size = 10\n",
    "trg_vocab_size = 10\n",
    "embed_size = 128\n",
    "num_layers = 2\n",
    "heads = 2\n",
    "forward_expansion = 4\n",
    "dropout = 0.1\n",
    "max_length = 9  # Length of your sequences\n",
    "\n",
    "# Sample training data\n",
    "x = tf.constant([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]], dtype= tf.float32)\n",
    "trg = tf.constant([[1, 7, 4, 3, 5, 9, 2, 0, 0], [1, 5, 6, 2, 4, 7, 6, 2, 0]], dtype= tf.float32)\n",
    "\n",
    "# Initialize the Transformer model\n",
    "model = Transformer(src_vocab_size, trg_vocab_size, src_pad_idx, trg_pad_idx, \n",
    "                    embed_size=embed_size, num_layers=num_layers, heads=heads, \n",
    "                    forward_expansion=forward_expansion, dropout=dropout, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trg_input = tf.cast(trg[:, :-1], dtype=tf.float32)  # Cast to float32\n",
    "trg_real = tf.cast(trg[:, 1:], dtype=tf.float32)    # Cast to float32\n",
    "x = tf.cast(x, dtype=tf.float32)                    # Cast source input to float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))  # Don't consider pad tokens in the loss\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = CustomSchedule(embed_size)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jites\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:607: UserWarning: `model.compiled_loss()` is deprecated. Instead, use `model.compute_loss(x, y, y_pred, sample_weight, training)`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Value passed to parameter 'x' has DataType int64 not in list of allowed values: bfloat16, float16, float32, float64, complex64, complex128",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 23\u001b[0m\n\u001b[0;32m     19\u001b[0m train_data \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(((x, trg_input), trg_real))\u001b[38;5;241m.\u001b[39mbatch(batch_size)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Train the model using the fit method\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# history = model.fit(src_mask, trg_mask, epochs=epochs)\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jites\\anaconda3\\envs\\myenv\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[1;32mIn[3], line 180\u001b[0m, in \u001b[0;36mTransformer.train_step\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    178\u001b[0m trainable_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_variables\n\u001b[0;32m    179\u001b[0m gradients \u001b[38;5;241m=\u001b[39m tape\u001b[38;5;241m.\u001b[39mgradient(loss, trainable_vars)\n\u001b[1;32m--> 180\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_gradients\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_vars\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;66;03m# Return metrics\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss}\n",
      "Cell \u001b[1;32mIn[7], line 9\u001b[0m, in \u001b[0;36mCustomSchedule.__call__\u001b[1;34m(self, step)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, step):\n\u001b[1;32m----> 9\u001b[0m     arg1 \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     arg2 \u001b[38;5;241m=\u001b[39m step \u001b[38;5;241m*\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwarmup_steps \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1.5\u001b[39m)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mrsqrt(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39md_model) \u001b[38;5;241m*\u001b[39m tf\u001b[38;5;241m.\u001b[39mmath\u001b[38;5;241m.\u001b[39mminimum(arg1, arg2)\n",
      "\u001b[1;31mTypeError\u001b[0m: Value passed to parameter 'x' has DataType int64 not in list of allowed values: bfloat16, float16, float32, float64, complex64, complex128"
     ]
    }
   ],
   "source": [
    "# Define some basic training hyperparameters\n",
    "epochs = 10\n",
    "batch_size = 2\n",
    "\n",
    "# Dummy masks for training (adjust if necessary for your real task)\n",
    "def create_masks(src, trg):\n",
    "    src_mask = model.make_src_mask(src)\n",
    "    trg_mask = model.make_trg_mask(trg)\n",
    "    return src_mask, trg_mask\n",
    "\n",
    "# Create masks and use them in the dataset if necessary\n",
    "src_mask = model.make_src_mask(x)\n",
    "trg_mask = model.make_trg_mask(trg)\n",
    "\n",
    "# # You can add a simple data pipeline if needed (e.g., batching, shuffling)\n",
    "# train_data = tf.data.Dataset.from_tensor_slices((x, trg)).batch(batch_size)\n",
    "\n",
    "# Prepare dataset for training\n",
    "train_data = tf.data.Dataset.from_tensor_slices(((x, trg_input), trg_real)).batch(batch_size)\n",
    "\n",
    "# Train the model using the fit method\n",
    "# history = model.fit(src_mask, trg_mask, epochs=epochs)\n",
    "history = model.fit(train_data, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training hyperparameters\n",
    "# epochs = 10\n",
    "# batch_size = 2\n",
    "\n",
    "# # Create masks for source and target data\n",
    "# def create_masks(src, trg):\n",
    "#     src_mask = model.make_src_mask(src)\n",
    "#     trg_mask = model.make_trg_mask(trg)\n",
    "#     return src_mask, trg_mask\n",
    "\n",
    "# # Training step function\n",
    "# @tf.function\n",
    "# def train_step(src, trg):\n",
    "#     trg_input = trg[:, :-1]\n",
    "#     trg_real = trg[:, 1:]\n",
    "\n",
    "#     src_mask, trg_mask = create_masks(src, trg_input)\n",
    "\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         predictions = model(src, trg_input)\n",
    "#         loss = loss_function(trg_real, predictions)\n",
    "\n",
    "#     gradients = tape.gradient(loss, model.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "#     return loss\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(epochs):\n",
    "#     total_loss = 0\n",
    "\n",
    "#     # Assume x and trg are your input and target batches\n",
    "#     batch_count = tf.shape(x)[0] // batch_size\n",
    "#     for i in range(batch_count):\n",
    "#         src_batch = x[i:i + batch_size]\n",
    "#         trg_batch = trg[i:i + batch_size]\n",
    "#         batch_loss = train_step(src_batch, trg_batch)\n",
    "#         total_loss += batch_loss\n",
    "\n",
    "#     print(f'Epoch {epoch + 1}, Loss: {total_loss / batch_count}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
